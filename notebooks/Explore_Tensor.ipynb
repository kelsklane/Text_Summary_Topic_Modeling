{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c0b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f7537a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Justin Lee</td>\n",
       "      <td>8.3K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://medium.com/swlh/chatbots-were-the-next...</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conor Dewey</td>\n",
       "      <td>1.4K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://towardsdatascience.com/python-for-data...</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>William Koehrsen</td>\n",
       "      <td>2.8K</td>\n",
       "      <td>11</td>\n",
       "      <td>https://towardsdatascience.com/automated-featu...</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gant Laborde</td>\n",
       "      <td>1.3K</td>\n",
       "      <td>7</td>\n",
       "      <td>https://medium.freecodecamp.org/machine-learni...</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emmanuel Ameisen</td>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>https://blog.insightdatascience.com/reinforcem...</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author claps  reading_time  \\\n",
       "0        Justin Lee  8.3K            11   \n",
       "1       Conor Dewey  1.4K             7   \n",
       "2  William Koehrsen  2.8K            11   \n",
       "3      Gant Laborde  1.3K             7   \n",
       "4  Emmanuel Ameisen   935            11   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://medium.com/swlh/chatbots-were-the-next...   \n",
       "1  https://towardsdatascience.com/python-for-data...   \n",
       "2  https://towardsdatascience.com/automated-featu...   \n",
       "3  https://medium.freecodecamp.org/machine-learni...   \n",
       "4  https://blog.insightdatascience.com/reinforcem...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Chatbots were the next big thing: what happene...   \n",
       "1  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2  Automated Feature Engineering in Python – Towa...   \n",
       "3  Machine Learning: how to go from Zero to Hero ...   \n",
       "4  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/articles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cfa92d",
   "metadata": {},
   "source": [
    "### Turn claps from str into int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d83cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_claps(num):\n",
    "    if num[-1] == 'K':\n",
    "        num = num[:-1]\n",
    "        num += '00'\n",
    "        num = num.replace('.', '') \n",
    "    return num\n",
    "df.claps = df.claps.apply(clean_up_claps)\n",
    "df.claps = df.claps.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67874496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8300</td>\n",
       "      <td>11</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1400</td>\n",
       "      <td>7</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2800</td>\n",
       "      <td>11</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>7</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claps  reading_time                                              title  \\\n",
       "0   8300            11  Chatbots were the next big thing: what happene...   \n",
       "1   1400             7  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2   2800            11  Automated Feature Engineering in Python – Towa...   \n",
       "3   1300             7  Machine Learning: how to go from Zero to Hero ...   \n",
       "4    935            11  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \n",
       "0  Oh, how the headlines blared:\\nChatbots were T...  \n",
       "1  If you’ve ever found yourself looking up the s...  \n",
       "2  Machine learning is increasingly moving from h...  \n",
       "3  If your understanding of A.I. and Machine Lear...  \n",
       "4  Want to learn about applied Artificial Intelli...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get rid of link column\n",
    "df = df.drop(labels = ['link', 'author'], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fb787",
   "metadata": {},
   "source": [
    "# Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03ff829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re\n",
    "\n",
    "punct = list(string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def bare_text(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.lower()\n",
    "    #Adds spaces where they are missing after punctuation\n",
    "    text = re.sub(r'(?<=[.,\\?!])(?=[^\\s])', r' ', text)\n",
    "    #Tokenize text\n",
    "    text_token = word_tokenize(text)\n",
    "    #Get rid of stopwords\n",
    "    text_token = [w for w in text_token if w.lower() not in sw]\n",
    "    #Lemmatize text\n",
    "    text_token = pos_tag(text_token)\n",
    "    text_token = [(w[0], pos_replace(w[1])) for w in text_token]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text_token = [lemmatizer.lemmatize(word[0], word[1]) for word in text_token]\n",
    "    #Get rid of punctuation\n",
    "    text_token = [w for w in text_token if w not in punct]\n",
    "    #Special punctuation marks not included in original list\n",
    "    text_token = [w for w in text_token if w not in [\"’\", \"-\", \"‘\"]]\n",
    "    text = TreebankWordDetokenizer().detokenize(text_token)\n",
    "    return text\n",
    "\n",
    "def word_tokens(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.lower()\n",
    "    #Adds spaces where they are missing after punctuation\n",
    "    text = re.sub(r'(?<=[.,\\?!])(?=[^\\s])', r' ', text)\n",
    "    #Tokenize text\n",
    "    text_token = word_tokenize(text)\n",
    "    #Get rid of stopwords\n",
    "    text_token = [w for w in text_token if w.lower() not in sw]\n",
    "    #Lemmatize text\n",
    "    text_token = pos_tag(text_token)\n",
    "    text_token = [(w[0], pos_replace(w[1])) for w in text_token]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text_token = [lemmatizer.lemmatize(word[0], word[1]) for word in text_token]\n",
    "    #Get rid of punctuation\n",
    "    text_token = [w for w in text_token if w not in punct]\n",
    "    #Special punctuation marks not included in original list\n",
    "    text_token = [w for w in text_token if w not in [\"’\", \"-\", \"‘\"]]\n",
    "    return text_token\n",
    "\n",
    "def clean_sentences(text):\n",
    "    text = text.replace('\\n','')\n",
    "    #Get rid of links\n",
    "    text = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', text)\n",
    "    #Add space after punctuation if its not there\n",
    "    text = re.sub(r'(?<=[.,\\?!:])(?=[^\\s])', r' ', text)\n",
    "    text = text.lower()\n",
    "    #Get rid of punctuation\n",
    "    text.replace(\"[^a-zA-Z]\", \" \").split(\" \")\n",
    "    sent = sent_tokenize(text)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def52fe",
   "metadata": {},
   "source": [
    "# Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c0843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sim(sent1, sent2, stopwords = None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ae59ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import*\n",
    " \n",
    "def sent_sim_jaccard(sent1, sent2, stopwords = None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "        \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        \n",
    "    intersection = len(set.intersection(*[set(vector1), set(vector2)]))\n",
    "    union = len(set.union(*[set(vector1), set(vector2)]))\n",
    "    return intersection/float(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5ea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(sent, stop_words = sw):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sent), len(sent)))\n",
    " \n",
    "    for ind1 in range(len(sent)):\n",
    "        for ind2 in range(len(sent)):\n",
    "            if ind1 == ind2:\n",
    "                continue \n",
    "            similarity_matrix[ind1][ind2] = sent_sim(sent[ind1], sent[ind2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5352e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix_j(sent, stop_words = sw):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sent), len(sent)))\n",
    " \n",
    "    for ind1 in range(len(sent)):\n",
    "        for ind2 in range(len(sent)):\n",
    "            if ind1 == ind2:\n",
    "                continue \n",
    "            similarity_matrix[ind1][ind2] = sent_sim_jaccard(sent[ind1], sent[ind2], stop_words)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c217ed2",
   "metadata": {},
   "source": [
    "# Summarize Text By Hand - Extractive Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43bc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article, top_n = 3):\n",
    "    summarize_text = []\n",
    "    sentences =  clean_sentences(article)\n",
    "    #Find similar sentences\n",
    "    sentence_sim_martix = sim_matrix(sentences)\n",
    "    sentence_sim_graph = nx.from_numpy_array(sentence_sim_martix)\n",
    "    scores = nx.pagerank(sentence_sim_graph)\n",
    "    #Rank similarity and find summary sentences\n",
    "    ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse = True)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(ranked_sentence[i][1])\n",
    "    summary = \" \".join(summarize_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aba3790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check out the simple example below and the upcoming video to get a better feel for the power of lambda functions: once you have a grasp on lambda functions, learning to pair them with the map and filter functions can be a powerful tool. the filter function takes in a list and a rule, much like map, however it returns a subset of the original list by comparing each element against the boolean filtering rule. personally, i find myself pulling code from similar discussion threads several times, rather than taking the time to learn and solidify the concept so that i can reproduce the code myself the next time.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.text[1]\n",
    "generate_summary(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ce7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_jaccard(article, top_n = 3):\n",
    "    summarize_text = []\n",
    "    sentences =  clean_sentences(article)\n",
    "    #Find similar sentences\n",
    "    sentence_sim_martix = sim_matrix_j(sentences)\n",
    "    sentence_sim_graph = nx.from_numpy_array(sentence_sim_martix)\n",
    "    scores = nx.pagerank(sentence_sim_graph)\n",
    "    #Rank similarity and find summary sentences\n",
    "    ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse = True)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(ranked_sentence[i][1])\n",
    "    summary = \" \".join(summarize_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4b7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'basically, they let you create a function, without creating a function. note that the stopping point is a ‘cut-off’ value, so it will not be included in the array output. think of apply as a map function, but made for pandas dataframes or more specifically, for series.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary_jaccard(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67fe9c",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ab3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from summarizer import Summarizer,TransformerSummarizer\n",
    "# def generate_summary(article, top_n = 3):\n",
    "#     bert_model = Summarizer()\n",
    "#     bert_summary = ''.join(bert_model(article, min_length=60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8b0d8",
   "metadata": {},
   "source": [
    "# Spacy Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca61c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def generate_summary_spacy(article, n_sent = 3):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    text = nlp(article)\n",
    "    tokens = [token.text for token in text]\n",
    "    word_frequencies = {}\n",
    "    for word in text:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "                    \n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "        \n",
    "    sentence_tokens= [sent for sent in text.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "                    \n",
    "    summary = nlargest(n_sent, sentence_scores, key = sentence_scores.get)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    summary = ''.join(final_summary)\n",
    "    summary = summary.replace('\\n','')\n",
    "    summary = re.sub(r'(?<=[.,\\?!:])(?=[^\\s])', r' ', summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "121bc400",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_summary_spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j9/ykvypybn0j5fft1yld3l1xrw0000gn/T/ipykernel_3157/3596516040.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_summary_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_summary_spacy' is not defined"
     ]
    }
   ],
   "source": [
    "test = df.text[1]\n",
    "generate_summary_spacy(test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740529af",
   "metadata": {},
   "source": [
    "# Sumy - LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56419bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "def generate_summary_lsa(article):\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer('english'))\n",
    "    lsa = LsaSummarizer()\n",
    "    lsa_summary = lsa(parser.document, 3)\n",
    "    summary = ''\n",
    "    for s in lsa_summary: \n",
    "        if summary == '':\n",
    "            summary = str(s)\n",
    "        else:\n",
    "            summary = summary + ' ' + str(s)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab42ab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Each one has their specific purpose, but the appeal here (instead of using range), is that they output NumPy arrays, which are typically easier to work with for data science. You might imagine how useful this can be, especially for formatting and manipulating values across a whole DataFrame column, without having to loop at all. I hope a couple of these overviews have effectively jogged your memory regarding important yet somewhat tricky methods, functions, and concepts you frequently encounter when using Python for data science.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary_lsa(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cfa5f5",
   "metadata": {},
   "source": [
    "# Sumy - LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6729fd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def generate_summary_lexrank(article):\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer('english'))\n",
    "    lex_rank = LexRankSummarizer() \n",
    "    lex_summary = lex_rank(parser.document, 3)\n",
    "    summary = ''\n",
    "    for s in lex_summary: \n",
    "        if summary == '':\n",
    "            summary = str(s)\n",
    "        else:\n",
    "            summary = summary + ' ' + str(s)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe313145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So given a starting and stopping point, as well as a number of values, linspace will evenly space them out for you in a NumPy array. Let’s use the example of dropping a column for now: I don’t know how many times I wrote this line of code before I actually knew why I was declaring axis what I was. If you think about how this is indexed in Python, rows are at 0 and columns are at 1, much like how we declare our axis value.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_summary_lexrank(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e7bb9",
   "metadata": {},
   "source": [
    "# Adds summary column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17eff6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for article in df.text:\n",
    "    summ = generate_summary_spacy(article)\n",
    "    summaries.append(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70c233b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7120169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/with_summary', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
