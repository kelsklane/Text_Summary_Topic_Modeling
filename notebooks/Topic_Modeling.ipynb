{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4628447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff119153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claps</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8300</td>\n",
       "      <td>11</td>\n",
       "      <td>Chatbots were the next big thing: what happene...</td>\n",
       "      <td>Oh, how the headlines blared:\\nChatbots were T...</td>\n",
       "      <td>Building a bot for the sake of it, letting it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1400</td>\n",
       "      <td>7</td>\n",
       "      <td>Python for Data Science: 8 Concepts You May Ha...</td>\n",
       "      <td>If you’ve ever found yourself looking up the s...</td>\n",
       "      <td>The basic syntax of lambda functions is: Note ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2800</td>\n",
       "      <td>11</td>\n",
       "      <td>Automated Feature Engineering in Python – Towa...</td>\n",
       "      <td>Machine learning is increasingly moving from h...</td>\n",
       "      <td>For example, we have the month each client joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1300</td>\n",
       "      <td>7</td>\n",
       "      <td>Machine Learning: how to go from Zero to Hero ...</td>\n",
       "      <td>If your understanding of A.I. and Machine Lear...</td>\n",
       "      <td>Software Consultant, Adjunct Professor, Publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>935</td>\n",
       "      <td>11</td>\n",
       "      <td>Reinforcement Learning from scratch – Insight ...</td>\n",
       "      <td>Want to learn about applied Artificial Intelli...</td>\n",
       "      <td>A note about off-policy vs on-policy learning:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claps  reading_time                                              title  \\\n",
       "0   8300            11  Chatbots were the next big thing: what happene...   \n",
       "1   1400             7  Python for Data Science: 8 Concepts You May Ha...   \n",
       "2   2800            11  Automated Feature Engineering in Python – Towa...   \n",
       "3   1300             7  Machine Learning: how to go from Zero to Hero ...   \n",
       "4    935            11  Reinforcement Learning from scratch – Insight ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Oh, how the headlines blared:\\nChatbots were T...   \n",
       "1  If you’ve ever found yourself looking up the s...   \n",
       "2  Machine learning is increasingly moving from h...   \n",
       "3  If your understanding of A.I. and Machine Lear...   \n",
       "4  Want to learn about applied Artificial Intelli...   \n",
       "\n",
       "                                             summary  \n",
       "0  Building a bot for the sake of it, letting it ...  \n",
       "1  The basic syntax of lambda functions is: Note ...  \n",
       "2  For example, we have the month each client joi...  \n",
       "3  Software Consultant, Adjunct Professor, Publis...  \n",
       "4  A note about off-policy vs on-policy learning:...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/with_summary')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e953385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re\n",
    "\n",
    "punct = list(string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def bare_text(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.lower()\n",
    "    #Adds spaces where they are missing after punctuation\n",
    "    text = re.sub(r'(?<=[.,\\?!])(?=[^\\s])', r' ', text)\n",
    "    #Tokenize text\n",
    "    text_token = word_tokenize(text)\n",
    "    #Get rid of stopwords\n",
    "    text_token = [w for w in text_token if w.lower() not in sw]\n",
    "    #Lemmatize text\n",
    "    text_token = pos_tag(text_token)\n",
    "    text_token = [(w[0], pos_replace(w[1])) for w in text_token]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text_token = [lemmatizer.lemmatize(word[0], word[1]) for word in text_token]\n",
    "    #Get rid of punctuation\n",
    "    text_token = [w for w in text_token if w not in punct]\n",
    "    #Special punctuation marks not included in original list\n",
    "    text_token = [w for w in text_token if w not in [\"’\", \"-\", \"‘\"]]\n",
    "    text = TreebankWordDetokenizer().detokenize(text_token)\n",
    "    return text\n",
    "\n",
    "def tokens(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.lower()\n",
    "    #Adds spaces where they are missing after punctuation\n",
    "    text = re.sub(r'(?<=[.,\\?!])(?=[^\\s])', r' ', text)\n",
    "    #Tokenize text\n",
    "    text_token = word_tokenize(text)\n",
    "    #Get rid of stopwords\n",
    "    text_token = [w for w in text_token if w.lower() not in sw]\n",
    "    #Lemmatize text\n",
    "    text_token = pos_tag(text_token)\n",
    "    text_token = [(w[0], pos_replace(w[1])) for w in text_token]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text_token = [lemmatizer.lemmatize(word[0], word[1]) for word in text_token]\n",
    "    #Get rid of punctuation\n",
    "    text_token = [w for w in text_token if w not in punct]\n",
    "    #Special punctuation marks not included in original list\n",
    "    text_token = [w for w in text_token if w not in [\"’\", \"-\", \"‘\"]]\n",
    "    return text_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ebad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary = df.summary.apply(bare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9737c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56afb0a",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERT\n",
    "---\n",
    "Code references from [here](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6549d",
   "metadata": {},
   "source": [
    "Generate text embeddings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54973949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720f01b9950e4d748babd2f4cdf66056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Batches'), FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(text.text, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5ba01",
   "metadata": {},
   "source": [
    "Dimentionality reduction using UMAP - optional if doing something that deals well with high dimentionality like k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "271b44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdaf6b",
   "metadata": {},
   "source": [
    "Cluster the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb7eb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34866b5",
   "metadata": {},
   "source": [
    "Class-based TF_IDF implementation (frequency by topic rather than document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2963c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Doc': df.text, 'Topic': cluster.labels_}\n",
    "docs_df = pd.DataFrame(data = d, columns = ['Doc', 'Topic'])\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a9509ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelseylane/miniforge3/envs/tensor/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, tokenizer = tokens).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b021f3",
   "metadata": {},
   "source": [
    "Topic representation, 20 words that represent each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b6c2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelseylane/miniforge3/envs/tensor/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Size\n",
       "0     -1   123\n",
       "3      2    94\n",
       "4      3    70\n",
       "2      1    31\n",
       "1      0    19"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
    "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
