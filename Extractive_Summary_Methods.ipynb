{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be2b06a5",
   "metadata": {},
   "source": [
    "# Overview\n",
    "---\n",
    "This notebook contains the code for the various extractive summary methods outlined in this article. Each method is sectioned into its' own area and the imports for everything are grouped below. All functions take in a string that represents the text of the entire article and return a string that contains the sentences that make up the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b541d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import re\n",
    "\n",
    "# Cosine Similarity\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import networkx as nx\n",
    "\n",
    "# Jaccard Similarity\n",
    "from math import *\n",
    "\n",
    "# SpaCy\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "# LSA\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "# LexRank\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "\n",
    "# Genism\n",
    "#from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c586b11c",
   "metadata": {},
   "source": [
    "### Preprocessing Helper Functions\n",
    "---\n",
    "While some of the methods involve libraries that automatically preprocess the text, other's still require that this step is done manually. The helper funciton used to clean the data I used for testing purposes is included below, though this would obviously be modified to fit the data that you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating lists of punctuation and stopwords to use later\n",
    "punct = list(string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#Function to replace part of speech tags\n",
    "def pos_replace(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Returns text as string after preprocessing\n",
    "def bare_text(text):\n",
    "    text = text.replace('\\n','')\n",
    "    text = text.lower()\n",
    "    #Adds spaces where they are missing after punctuation\n",
    "    text = re.sub(r'(?<=[.,\\?!])(?=[^\\s])', r' ', text)\n",
    "    #Tokenize and lemmatize text\n",
    "    text_token = word_tokenize(text)\n",
    "    text_token = [w for w in text_token if w.lower() not in sw]\n",
    "    text_token = pos_tag(text_token)\n",
    "    text_token = [(w[0], pos_replace(w[1])) for w in text_token]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text_token = [lemmatizer.lemmatize(word[0], word[1]) for word in text_token]\n",
    "    #Get rid of punctuation\n",
    "    text_token = [w for w in text_token if w not in punct]\n",
    "    text_token = [w for w in text_token if w not in [\"’\", \"-\", \"‘\"]]\n",
    "    #Reconcatenate tokens\n",
    "    text = TreebankWordDetokenizer().detokenize(text_token)\n",
    "    return text\n",
    "\n",
    "#Returns list of sentences where each sentence is preprocessed\n",
    "def clean_sentences(text):\n",
    "    text = text.replace('\\n','')\n",
    "    #Get rid of links\n",
    "    text = re.sub(r'www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)', '', text)\n",
    "    #Add space after punctuation if its not there\n",
    "    text = re.sub(r'(?<=[.,\\?!:])(?=[^\\s])', r' ', text)\n",
    "    text = text.lower()\n",
    "    #Get rid of punctuation\n",
    "    text.replace(\"[^a-zA-Z]\", \" \").split(\" \")\n",
    "    sent = sent_tokenize(text)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff75207",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "---\n",
    "The first summary method used is based on cosine similarity. The first two functions are helper methods used to calculate the similarity between each sentence based on their cosine similarity and then generate a matrix of similarity between each sentence. The final method, `generate_summary_cosine`, is the actual summary generation method where top_n is the number of sentences to include in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7aecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns similarity between two sentences\n",
    "def sent_sim(sent1, sent2, stopwords = None, method):\n",
    "    #Filter out stopwords for the sentences\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "   \n",
    "    #Makes a list of all the words and builds vectors for each sentence\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    #Builds the vector for the first sentence\n",
    "    for word in sent1:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(word)] += 1\n",
    " \n",
    "    #Builds the vector for the second sentence\n",
    "    for word in sent2:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(word)] += 1\n",
    "    \n",
    "    #Computes similarity based on which metric is used\n",
    "    if method == 'cosine':\n",
    "        return 1 - cosine_distance(vector1, vector2)\n",
    "    else:\n",
    "        intersection = len(set.intersection(*[set(vector1), set(vector2)]))\n",
    "        union = len(set.union(*[set(vector1), set(vector2)]))\n",
    "        return intersection / float(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes similarity matrix for all sentences\n",
    "def sim_matrix(sent, stopwords = sw, method):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sent), len(sent)))\n",
    "    \n",
    "    #Calculate similarity for each sentence pairing\n",
    "    for ind1 in range(len(sent)):\n",
    "        for ind2 in range(len(sent)):\n",
    "            if ind1 == ind2:\n",
    "                continue \n",
    "            similarity_matrix[ind1][ind2] = sent_sim(sent[ind1], sent[ind2], stop_words, method)\n",
    "            \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f854100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates summary based on sentence similarity\n",
    "def generate_summary_cosine(article, top_n = 3):\n",
    "    #Cleans text and prepares list for summary sentences\n",
    "    summarize_text = []\n",
    "    sentences = clean_sentences(article)\n",
    "    \n",
    "    #Find similar sentences\n",
    "    sentence_sim_martix = sim_matrix(sentences, method = 'cosine')\n",
    "    sentence_sim_graph = nx.from_numpy_array(sentence_sim_martix)\n",
    "    scores = nx.pagerank(sentence_sim_graph)\n",
    "    \n",
    "    #Rank similarity and find best n summary sentences\n",
    "    ranked_sentence = sorted(((scores[i], sent) for i, sent in enumerate(sentences)), reverse = True)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(ranked_sentence[i][1])\n",
    "        \n",
    "    summary = \" \".join(summarize_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c47a6b",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "---\n",
    "The next summary method used is based on Jaccard similarity. The first two helper functions for cosine similarity are again used here as there is a lot of overlap in the code, and thus they can be referenced above. The final summary method also has a lot of overlap with the cosine summary method, but is left here as a reference. Again, top_n can be used to determine how many sentences are returned for the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1138ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates summary based on sentence similarity\n",
    "def generate_summary_jaccard(article, top_n = 3):\n",
    "    #Cleans text and prepares list for summary sentences\n",
    "    summarize_text = []\n",
    "    sentences =  clean_sentences(article)\n",
    "    \n",
    "    #Find similar sentences\n",
    "    sentence_sim_martix = sim_matrix(sentences, method = 'jaccard')\n",
    "    sentence_sim_graph = nx.from_numpy_array(sentence_sim_martix)\n",
    "    scores = nx.pagerank(sentence_sim_graph)\n",
    "    \n",
    "    #Rank similarity and find best n summary sentences\n",
    "    ranked_sentence = sorted(((scores[i], sent) for i, sent in enumerate(sentences)), reverse = True)    \n",
    "    for i in range(top_n):\n",
    "        summarize_text.append(ranked_sentence[i][1])\n",
    "        \n",
    "    summary = \" \".join(summarize_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f0fff",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "---\n",
    "Another summary method discussed in the article is one based in the SpaCy library. This method involves finding the sentences for the summaries based on which sentences contain the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates summary using SpaCy library\n",
    "def generate_summary_spacy(article, top_n = 3):\n",
    "    #Loads pipeline and instantiates article as nlp object\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    text = nlp(article)\n",
    "    \n",
    "    #Tokenizes text and generates word frequencies\n",
    "    tokens = [token.text for token in text]\n",
    "    word_frequencies = {}\n",
    "    for word in text:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "                    \n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "    \n",
    "    #Scores each sentence based on word frequencies\n",
    "    sentence_tokens= [sent for sent in text.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "                    \n",
    "    #Finds n sentences with the most frequent words/highest score                 \n",
    "    summary = nlargest(top_n, sentence_scores, key = sentence_scores.get)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    summary = ''.join(final_summary)\n",
    "    \n",
    "    #Filters out characters not handeled by SpaCy\n",
    "    summary = summary.replace('\\n','')\n",
    "    summary = re.sub(r'(?<=[.,\\?!:])(?=[^\\s])', r' ', summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df4afc",
   "metadata": {},
   "source": [
    "### LSA\n",
    "---\n",
    "Latent Semantic Analysis (LSA) is another summary method that can be used. In this case it is implemented through the use of sumy, a module designed for automatic summarization. Various other summary methods not mentioned in the article are also availible in this module, though another example using LexRank from this same module is included below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates summary using LSA\n",
    "def generate_summary_lsa(article, top_n = 3):\n",
    "    #Tokenizes text and summarizes using LSA\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer('english'))\n",
    "    lsa = LsaSummarizer()\n",
    "    lsa_summary = lsa(parser.document, top_n)\n",
    "    \n",
    "    #Joins sentences together\n",
    "    summary = ''\n",
    "    for s in lsa_summary: \n",
    "        if summary == '':\n",
    "            summary = str(s)\n",
    "        else:\n",
    "            summary = summary + ' ' + str(s)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f265e9b",
   "metadata": {},
   "source": [
    "### LexRank\n",
    "---\n",
    "Finally, the last method mentioned in the article is one that uses LexRank through the sumy module. It is executed much the same as the LSA method above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38946a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates summary using LexRank\n",
    "def generate_summary_lexrank(article, top_n = 3):\n",
    "    #Tokenizes text and summarizes using LexRank\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer('english'))\n",
    "    lex_rank = LexRankSummarizer() \n",
    "    lex_summary = lex_rank(parser.document, top_n)\n",
    "    \n",
    "    #Joins sentences together\n",
    "    summary = ''\n",
    "    for s in lex_summary: \n",
    "        if summary == '':\n",
    "            summary = str(s)\n",
    "        else:\n",
    "            summary = summary + ' ' + str(s)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692061c0",
   "metadata": {},
   "source": [
    "### Bonus: Genism\n",
    "---\n",
    "While not explicitly talked through in the article, Genism is another popular NLP library that has it's own pre-built summary method. The example code for how to execute a summary using Genism can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2fb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate summay with n word_count\n",
    "def generate_summary_genism(article, word_count = 250):\n",
    "    #Preprocesses text\n",
    "    sentences = clean_sentences(article)\n",
    "    text = \" \".join(sentences)\n",
    "    \n",
    "    #Generates summary with n words based on given word_count\n",
    "    summary = summarize(text, word_count = word_count)\n",
    "    summary = summary.replace('\\n',' ')\n",
    "    return summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
